{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# CandidateGPT üá∫üá∏"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CandidateGPT** is a python-based chatbot built around eight U.S. presidential candidates to allow a user to ask questions about a candidate's political positions. Each candidate has a corresponding embeddings vector database based on publicly available data found on the candidate's corresponding wikipedia page. A subset of candidates was selected from the Wikipedia page, [\"2024_United_States_presidential_election\"](https://en.wikipedia.org/wiki/2024_United_States_presidential_election), upon which to build the various vector databases.\n",
        "\n",
        "## Disclaimer:\n",
        "‚ö†Ô∏è IMPORTANT: This project, the code in it, and outputs, are NOT IN ANY WAY an endorsement of any of the following candidates, their policies, or beliefs. This project was solely an experiment into using multiple vector databases with embeddings to practice semantic text search querying against OpenAI's `text-davinci-003` completion model for generative AI text generation.\n",
        "\n",
        "## Use-Cases:\n",
        "- Utilizing generative AI to increase civic engagement\n",
        "- Augment a given voter's understandings about the policy positions of their candidates\n",
        "- Provide concise, right-to-the-point, explanations about complex issues\n",
        "- Improve AC (Augmented Communication) between voters and the candidates\n",
        "\n",
        "## Tools used:\n",
        "- AI API: `OpenAI`\n",
        "- Data Manipulation Library: `Pandas`\n",
        "- Development/Documentation: `Jupyter Notebook`\n",
        "- Model for Completions: `text-davinci-003`\n",
        "- Model for Embeddings: `text-embedding-ada-002`\n",
        "- Programming Language: `Python`\n",
        "- Tokenizer: `tokenizer`\n",
        "- Data Sources: `Wikipedia`\n",
        "\n",
        "## Data sources:\n",
        "- [Joe Biden](https://en.wikipedia.org/wiki/Joe_Biden)\n",
        "- [Ron DeSantis](https://en.wikipedia.org/wiki/Ron_DeSantis)\n",
        "- [Nikki Haley](https://en.wikipedia.org/wiki/Nikki_Haley)\n",
        "- [Robert F. Kennedy Jr.](https://en.wikipedia.org/wiki/Robert_F._Kennedy_Jr.)\n",
        "- [Mike Pence](https://en.wikipedia.org/wiki/Mike_Pence)\n",
        "- [Vivek Ramaswamy](https://en.wikipedia.org/wiki/Vivek_Ramaswamy)\n",
        "- [Donald Trump](https://en.wikipedia.org/wiki/Donald_Trump)\n",
        "- [Marianne Williamson](https://en.wikipedia.org/wiki/Marianne_Williamson)\n",
        "\n",
        "## Code Details\n",
        "- To gather an initial dataset, each candidate's Wikipedia page is loaded into an `embeddings.csv` file with 3 columns:\n",
        "    - index\n",
        "    - text\n",
        "    - embeddings\n",
        "- To order the dataset based on those embeddings, each `embeddings.csv` file is transformed into a `distances.csv` file with 4 columns:\n",
        "    - index\n",
        "    - text\n",
        "    - embeddings\n",
        "    - distances\n",
        "- The distances are what will be used to allow our model to find data that is most relevant as \"context\" when submitting the Zero Shot Prompt into the generative text model.\n",
        "- For data wrangling, each candidate's dataset is loaded into a `pandas` dataframe with 4 columns: \n",
        "    - index\n",
        "    - text\n",
        "    - embeddings\n",
        "    - distances\n",
        "- The user then must pass in 2 values to a custom query along with a for-looped context array to OpenAI for custom query completion:\n",
        "    1. `SELECTED_PARAMS_CANDIDATE`\n",
        "        - This is a camelcase variable listed in the `candidate_params` Python dictionary\n",
        "        - This variable is connected to the `SELECTED_PARAMS_CANDIATE_FULLNAME` variable which looks up its corresonding value in the `candiate_lookup` directory for the \"fullName\" of that candidate\n",
        "    2. `USER_QUESTION_INPUT` \n",
        "        - This is a camelcase variable listed in the `candidate_questions_testing` Python dictionary\n",
        "        - I used this dictionary for testing, but users could type in their own question as well.\n",
        "\n",
        "## Scenario Details + Dataset Considerations\n",
        "- In the Jupyter Notebook code below, it shows 2 presidential candidates Joe Biden and Nikki Haley, being asked about their stances on `discrimination` and `foreign policy`.\n",
        "    - ‚ùå Without vector database training the model:\n",
        "        - Example #1 Pt.1: Nikki Haley (UNTRAINED) - discrimination\n",
        "            - Couldn't generate a response (lack of context?)\n",
        "        - Example #1 Pt.2: Nikki Haley (UNTRAINED) - foreign policy\n",
        "            - Couldn't generate a response (lack of context?)\n",
        "\n",
        "        - Example #2 Pt.1: Joe Biden (UNTRAINED) - discrimination\n",
        "            - Answer is very general/broad\n",
        "        - Example #2 Pt.2: Joe Biden (UNTRAINED) - foreign policy\n",
        "            - Answer is very general/broad\n",
        "    - ‚úÖ With vector database training the model:\n",
        "        - Example #3: Nikki Haley (TRAINED) - discrimination\n",
        "            - Answer is crisp, clear, and detail-oriented\n",
        "        - Example #4: Joe Biden (TRAINED) - discrimination\n",
        "            - Answer is crisp, clear, and detail-oriented\n",
        "\n",
        "        - Example #5: Nikki Haley (TRAINED) - foreign policy\n",
        "            - Answer is crisp, clear, and detail-oriented\n",
        "        - Example #6: Joe Biden (TRAINED) - foreign policy\n",
        "            - Answer is crisp, clear, and detail-oriented\n",
        "    \n",
        "- At the end of the Jupyter Notebook, I created a \n",
        "- For this application, I created separate vector databases for each of the candidates for the following reasons:\n",
        "    - Since OpenAI's models are only trained up until 2021, there needs to be a way to fill in the gaps of the model should there not be data available to return a response\n",
        "    - Since there isn't already that additional context (because the candidates are running for president in 2024 and the model wouldn't have that yet), we can easily pull data from Wikipedia's API to generate a .csv file for our vector database of each candidate.\n",
        "    - I then map over the distance-sorted embeddings to put in the context up until the token limit that the model will allow. This is important so that I can send the max amount of data possible to the model for background context so that it can sort it, add it to its schema, and then return a well-worded and articulated response back to the user who asked the question.\n",
        "    - An assumption I made is that instead of jamming all of the candidates' data into one database, creating separate databases for each candidate would increase reliability of answers and speed of delivery, while cutting down on hallucinations wherever possible. \n",
        "        - I assume that as the data grows, it would be too burdonsome to re-create a mega-embeddings file, rather than creating embeddings and related vector embeddings distance files for each candidate in consideration. \n",
        "        - Also, since the prompts are designed to be one candidate + one topic per query, there would not be a need to cross-over and compare different candidates' datasets.\n",
        "\n",
        "## Cost Considerations\n",
        "- When working with tokens, especially in a testing setting, its easy to forget how many times a prompt was run or sent to the OpenAI API. So far, to run this lab, including the setup, tests, and shipping of this project, the total OpenAI costs have reached $1.91. To navigate around costs, these are some strategies I implemented while I built this project:\n",
        "    - Tried to print out values, dictionaries, and outputs without using any openai methods as much as possible\n",
        "    - Tried to test very small inputs to the model when I was testing\n",
        "    - Kept the OpenAI usage graph open and checked the billing after almost every call to see how various calls impacted price\n",
        "    - Included a price limit to my organization account to ensure there couldn't be a very large bill all of a sudden"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Add in Your OpenAI API Key"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You will need the following software packages to run the Jupyter workspace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install openai pandas tiktoken"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replace \"Your API Key\" with a valid OpenAI API key value and put it inside of the quotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"YOUR API KEY\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Review Non-Trained Results with OpenAI\n",
        "In this step, we will review the zero shot prompts fed into OpenAI directly without any vector embeddings or context sent to the model. We need to establish a baseline of the model not knowing \"what the candidate would be thinking\" without our dataset upon which the model would be relying on to answer the questions.\n",
        "\n",
        "The prompt we want to be able to pass into the chatbot include 3 parts:\n",
        "- A base prompt template\n",
        "- The user's selected candidate\n",
        "- The user's selected question\n",
        "\n",
        "We need a way to allow the user to pass in their candidate of choice to ask about, as well as a question about their policy position into a zero shot prompt that will go up to OpenAI and pass back a response. We will not be providing any context for testing purposes, so let's see what OpenAI responses with."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example #1: Nikki Haley (UNTRAINED) - Discrimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hmmm...I haven't heard their stance on this. Sorry!\n"
          ]
        }
      ],
      "source": [
        "TEST_PROMPT1_CONTEXT = {}\n",
        "TEST_PROMPT1_CANDIDATE = \"Nikki Haley\"\n",
        "TEST_PROMPT1_QUESTION = \"discrimination or equal opportunity\"\n",
        "\n",
        "test_prompt1_without_data = \"\"\"\n",
        "Answer the question based on the context below about the candidate's political stance, and if the question can't be answered based on the context, say \"Hmmm...I haven't heard their stance on this. Sorry!\"\n",
        "\n",
        "Context: \n",
        "\n",
        "{}\n",
        "\n",
        "---\n",
        "\n",
        "Question: \"What is {}'s stance on {}?\"\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "initial_test_prompt1_answer = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=test_prompt1_without_data.format(TEST_PROMPT1_CONTEXT, TEST_PROMPT1_CANDIDATE, TEST_PROMPT1_QUESTION),\n",
        "    max_tokens=150\n",
        ")[\"choices\"][0][\"text\"].strip()\n",
        "print(initial_test_prompt1_answer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example #1 Pt.2: Nikki Haley (UNTRAINED) - Foreign Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hmmm...I haven't heard their stance on this. Sorry!\n"
          ]
        }
      ],
      "source": [
        "TEST_PROMPT11_CONTEXT = {}\n",
        "TEST_PROMPT11_CANDIDATE = \"Nikki Haley\"\n",
        "TEST_PROMPT11_QUESTION = \"foreign policy\"\n",
        "\n",
        "test_prompt11_without_data = \"\"\"\n",
        "Answer the question based on the context below about the candidate's political stance, and if the question can't be answered based on the context, say \"Hmmm...I haven't heard their stance on this. Sorry!\"\n",
        "\n",
        "Context: \n",
        "\n",
        "{}\n",
        "\n",
        "---\n",
        "\n",
        "Question: \"What is {}'s stance on {}?\"\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "initial_test_prompt11_answer = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=test_prompt11_without_data.format(TEST_PROMPT11_CONTEXT, TEST_PROMPT11_CANDIDATE, TEST_PROMPT11_QUESTION),\n",
        "    max_tokens=150\n",
        ")[\"choices\"][0][\"text\"].strip()\n",
        "print(initial_test_prompt11_answer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example #2: Joe Biden (UNTRAINED) - Discrimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joe Biden has advocated for equal opportunity and has publicly denounced discrimination. He has criticized issues such as voter suppression, calling it an affront to democracy, and has worked to earn the support of both sides of the aisle to promote equality.\n"
          ]
        }
      ],
      "source": [
        "TEST_PROMPT2_CONTEXT = {}\n",
        "TEST_PROMPT2_CANDIDATE = \"Joe Biden\"\n",
        "TEST_PROMPT2_QUESTION = \"discrimination or equal opportunity\"\n",
        "\n",
        "test_prompt2_without_data = \"\"\"\n",
        "Answer the question based on the context below about the candidate's political stance, and if the  question can't be answered based on the context, say \"Hmmm...I haven't heard their stance on this. Sorry!\"\n",
        "\n",
        "Context: \n",
        "\n",
        "{}\n",
        "\n",
        "---\n",
        "\n",
        "Question: \"What is {}'s stance on {}?\"\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "initial_test_prompt2_answer = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=test_prompt2_without_data.format(TEST_PROMPT2_CONTEXT, TEST_PROMPT2_CANDIDATE, TEST_PROMPT2_QUESTION),\n",
        "    max_tokens=150\n",
        ")[\"choices\"][0][\"text\"].strip()\n",
        "print(initial_test_prompt2_answer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example #2 Pt.2: Joe Biden (UNTRAINED) - Foreign Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joe Biden believes in upholding strong alliances and restoring America's credibility and influence on the world stage. He believes in the necessity of investing in diplomacy and development and engaging with the world to increase security against threats like terrorism, nuclear proliferation, climate change and cyber warfare.\n"
          ]
        }
      ],
      "source": [
        "TEST_PROMPT22_CONTEXT = {}\n",
        "TEST_PROMPT22_CANDIDATE = \"Joe Biden\"\n",
        "TEST_PROMPT22_QUESTION = \"foreign policy\"\n",
        "\n",
        "test_prompt22_without_data = \"\"\"\n",
        "Answer the question based on the context below about the candidate's political stance, and if the  question can't be answered based on the context, say \"Hmmm...I haven't heard their stance on this. Sorry!\"\n",
        "\n",
        "Context: \n",
        "\n",
        "{}\n",
        "\n",
        "---\n",
        "\n",
        "Question: \"What is {}'s stance on {}?\"\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "initial_test_prompt22_answer = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=test_prompt22_without_data.format(TEST_PROMPT22_CONTEXT, TEST_PROMPT22_CANDIDATE, TEST_PROMPT22_QUESTION),\n",
        "    max_tokens=150\n",
        ")[\"choices\"][0][\"text\"].strip()\n",
        "print(initial_test_prompt22_answer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Review Trained Results with OpenAI and Cosine Distance-Based Vector Database\n",
        "In this step, we will map our vector embeddings database against questions about that candidate so the user will receive unsupervised-learning OpenAI generated responses trained on our candidate databases.\n",
        "\n",
        "The prompt we want to be able to pass into the chatbot include 4 parts:\n",
        "- A base prompt template\n",
        "- The user's selected candidate\n",
        "- The user's selected question\n",
        "- The `distances.csv` file which uses cosine difference to group similar items together\n",
        "\n",
        "We need a way to allow the user to pass in their candidate of choice to ask about, as well as a question about their policy position into a zero shot prompt that will go up to OpenAI and pass back a response. We will be providing context in our zero shot prompt based on our cosine difference embeddings dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "token_limit = 4090\n",
        "\n",
        "candidate_params = {\n",
        "    \"joeBiden\": {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exlimit\": 1,\n",
        "        \"titles\": \"Joe_Biden\",\n",
        "        \"explaintext\": 1,\n",
        "        \"formatversion\": 2,\n",
        "        \"format\": \"json\"\n",
        "    },\n",
        "    \"robertFKennedyJr\": {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exlimit\": 1,\n",
        "        \"titles\": \"Robert_F._Kennedy_Jr.\",\n",
        "        \"explaintext\": 1,\n",
        "        \"formatversion\": 2,\n",
        "        \"format\": \"json\"\n",
        "    },\n",
        "    \"marianneWilliamson\": {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exlimit\": 1,\n",
        "        \"titles\": \"Marianne_Williamson\",\n",
        "        \"explaintext\": 1,\n",
        "        \"formatversion\": 2,\n",
        "        \"format\": \"json\"\n",
        "    },\n",
        "    \"nikkiHaley\": {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exlimit\": 1,\n",
        "        \"titles\": \"Nikki_Haley\",\n",
        "        \"explaintext\": 1,\n",
        "        \"formatversion\": 2,\n",
        "        \"format\": \"json\"\n",
        "    },\n",
        "    \"ronDeSantis\": {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exlimit\": 1,\n",
        "        \"titles\": \"Ron_DeSantis\",\n",
        "        \"explaintext\": 1,\n",
        "        \"formatversion\": 2,\n",
        "        \"format\": \"json\"\n",
        "    },\n",
        "    \"vivekRamaswamy\": {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exlimit\": 1,\n",
        "        \"titles\": \"Vivek_Ramaswamy\",\n",
        "        \"explaintext\": 1,\n",
        "        \"formatversion\": 2,\n",
        "        \"format\": \"json\"\n",
        "    },\n",
        "    \"donaldTrump\": {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exlimit\": 1,\n",
        "        \"titles\": \"Donald_Trump\",\n",
        "        \"explaintext\": 1,\n",
        "        \"formatversion\": 2,\n",
        "        \"format\": \"json\"\n",
        "    },\n",
        "    \"mikePence\": {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exlimit\": 1,\n",
        "        \"titles\": \"Mike_Pence\",\n",
        "        \"explaintext\": 1,\n",
        "        \"formatversion\": 2,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "}\n",
        "\n",
        "candiate_lookup = {\n",
        "    \"joeBiden\": {\n",
        "        \"fullName\": \"Joe Biden\"\n",
        "    },\n",
        "    \"robertFKennedyJr\": {\n",
        "        \"fullName\": \"Robert F. Kennedy Jr.\"\n",
        "    },\n",
        "    \"marianneWilliamson\": {\n",
        "        \"fullName\": \"Marianne Williamson\"\n",
        "    },\n",
        "    \"nikkiHaley\": {\n",
        "        \"fullName\": \"Nikki Haley\"\n",
        "    },\n",
        "    \"ronDeSantis\": {\n",
        "        \"fullName\": \"Ron DeSantis\"\n",
        "    },\n",
        "    \"vivekRamaswamy\": {\n",
        "        \"fullName\": \"Vivek Ramaswamy\"\n",
        "    },\n",
        "    \"donaldTrump\": {\n",
        "        \"fullName\": \"Donald Trump\"\n",
        "    },\n",
        "    \"mikePence\": {\n",
        "        \"fullName\": \"Mike Pence\"\n",
        "    }\n",
        "}\n",
        "\n",
        "candidate_questions_testing = {\n",
        "    \"topPoints\": \"top campaign agenda points\",\n",
        "    \"schoolChoice\": \"school choice\",\n",
        "    \"environment\": \"the environment or climate change\",\n",
        "    \"discrimination\": \"discrimination or equal opportunity\",\n",
        "}\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
        "COMPLETION_MODEL_NAME = \"text-davinci-003\"\n",
        "\n",
        "USER_QUESTION = \"\"\"What is {}'s stance on {}?\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example #3: Nikki Haley (TRAINED) - Discrimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nimarata Nikki Haley is a strong advocate for equal opportunity and is vehemently against all forms of discrimination. She has spoken out in support of LGBTQ rights, including supporting a bill in South Carolina that would open up employment protection for LGBTQ individuals. Additionally, she is an outspoken proponent of gender and racial equality, and has pushed for legislation to protect minority groups in her home state of South Carolina.\n"
          ]
        }
      ],
      "source": [
        "SELECTED_PARAMS3_CANDIDATE = \"nikkiHaley\"\n",
        "\n",
        "SELECTED_PARAMS3 = candidate_params[SELECTED_PARAMS3_CANDIDATE]\n",
        "SELECTED_PARAMS3_CANDIATE_FULLNAME = candiate_lookup[SELECTED_PARAMS3_CANDIDATE][\"fullName\"]\n",
        "\n",
        "USER_QUESTION_INPUT3 = candidate_questions_testing[\"discrimination\"]\n",
        "USER_QUESTION_FORMATTED3 = USER_QUESTION.format(\n",
        "    SELECTED_PARAMS3_CANDIATE_FULLNAME, USER_QUESTION_INPUT3)\n",
        "\n",
        "df3 = pd.read_csv(\n",
        "    \"distances/{}_distances.csv\".format(SELECTED_PARAMS3_CANDIDATE), index_col=0)\n",
        "\n",
        "prompt3_with_data = \"\"\"\n",
        "Answer the question based on the context below about the candidate's political stance, and if the question can't be answered based on the context, say \"Hmmm...I haven't heard their stance on this. Sorry!\"\n",
        "\n",
        "Context: \n",
        "\n",
        "{}\n",
        "\n",
        "---\n",
        "\n",
        "Question: {}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "token_count3 = len(tokenizer.encode(prompt3_with_data)) + len(tokenizer.encode(\n",
        "    USER_QUESTION_FORMATTED3\n",
        "))\n",
        "\n",
        "context_list3 = []\n",
        "\n",
        "for text in df3[\"text\"].values:\n",
        "    token_count3 += len(tokenizer.encode(text))\n",
        "    if token_count3 <= token_limit:\n",
        "        context_list3.append(text)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "prompt3 = prompt3_with_data.format(\n",
        "    \"\\n\\n###\\n\\n\".join(context_list3),\n",
        "    USER_QUESTION_FORMATTED3\n",
        ")\n",
        "# print(prompt3)\n",
        "\n",
        "prompt3_answer = openai.Completion.create(\n",
        "    model=COMPLETION_MODEL_NAME,\n",
        "    prompt=prompt3,\n",
        "    max_tokens=150\n",
        ")\n",
        "\n",
        "answer3 = prompt3_answer[\"choices\"][0][\"text\"].strip()\n",
        "print(answer3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example #4: Joe Biden (TRAINED) - Discrimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joe Biden strongly supports equal opportunity and opposes any forms of discrimination. He has proposed long-term reforms to ensure that minority and marginalized groups have the same access to education, healthcare, and the political process. He also strongly supports the protection of civil rights and human rights, as seen in his support for the Freedom to Vote Act, John Lewis Voting Rights Act, the American Rescue Plan Act of 2021, and his pledge to codify the protections of Roe v. Wade into federal law.\n"
          ]
        }
      ],
      "source": [
        "SELECTED_PARAMS4_CANDIDATE = \"joeBiden\"\n",
        "\n",
        "SELECTED_PARAMS4 = candidate_params[SELECTED_PARAMS4_CANDIDATE]\n",
        "SELECTED_PARAMS4_CANDIATE_FULLNAME = candiate_lookup[SELECTED_PARAMS4_CANDIDATE][\"fullName\"]\n",
        "\n",
        "USER_QUESTION_INPUT4 = candidate_questions_testing[\"discrimination\"]\n",
        "USER_QUESTION_FORMATTED4 = USER_QUESTION.format(\n",
        "    SELECTED_PARAMS4_CANDIATE_FULLNAME, USER_QUESTION_INPUT4)\n",
        "\n",
        "df4 = pd.read_csv(\n",
        "    \"distances/{}_distances.csv\".format(SELECTED_PARAMS4_CANDIDATE), index_col=0)\n",
        "\n",
        "prompt4_with_data = \"\"\"\n",
        "Answer the question based on the context below about the candidate's political stance, and if the question can't be answered based on the context, say \"Hmmm...I haven't heard their stance on this. Sorry!\"\n",
        "\n",
        "Context: \n",
        "\n",
        "{}\n",
        "\n",
        "---\n",
        "\n",
        "Question: {}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "token_count4 = len(tokenizer.encode(prompt4_with_data)) + len(tokenizer.encode(\n",
        "    USER_QUESTION_FORMATTED4\n",
        "))\n",
        "\n",
        "context_list4 = []\n",
        "\n",
        "for text in df4[\"text\"].values:\n",
        "    token_count4 += len(tokenizer.encode(text))\n",
        "    if token_count4 <= token_limit:\n",
        "        context_list4.append(text)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "prompt4 = prompt4_with_data.format(\n",
        "    \"\\n\\n###\\n\\n\".join(context_list4),\n",
        "    USER_QUESTION_FORMATTED4\n",
        ")\n",
        "# print(prompt3)\n",
        "\n",
        "prompt4_answer = openai.Completion.create(\n",
        "    model=COMPLETION_MODEL_NAME,\n",
        "    prompt=prompt4,\n",
        "    max_tokens=150\n",
        ")\n",
        "\n",
        "answer4 = prompt4_answer[\"choices\"][0][\"text\"].strip()\n",
        "print(answer4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4: Implement Repeatable Entries like a ChatBot for Better User Communications\n",
        "In this step, we will change our `for` loop to instead use a `while` loop so that the user can type their questions again, and again, without needing to edit the string content in the code cells every time.\n",
        "\n",
        "This allows for more of the experience we would get with a chatbot, such as the question-answer-question-answer, etc. type interactivity.\n",
        "\n",
        "Note: the candidate name you pass in at the first question MUST match the exact spelling listed in `candidate_params` dictionary, which in this case has 2 valid potential entries:\n",
        "    - `joeBiden`\n",
        "    - `nikkiHaley`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example #5: While Loop\n",
        "Code output is candidate Nikki Haley asked about Foreign Policy trained on the vector database in /distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nikki Haley has expressed her support of US interests, particularly in relation to Israel, and has been critical of the UN for its perceived anti-Israel bias. She has also championed the withdrawal of the US from the United Nations Human Rights Council, and has shown her willingness to use military force to respond to further North Korean missile tests. Haley has also criticized Trump's rhetoric which could lead to violent tragedy.\n"
          ]
        }
      ],
      "source": [
        "prompt_WHILE_with_data = \"\"\"\n",
        "Answer the question based on the context below about the candidate's political stance, and if the question can't be answered based on the context, say \"Hmmm...I haven't heard their stance on this. Sorry!\"\n",
        "\n",
        "Context: \n",
        "\n",
        "{}\n",
        "\n",
        "---\n",
        "\n",
        "Question: {}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    SELECTED_CANDIDATE_WHILE = input(\"Enter the name of the candidate (or 'quit' to stop): \")\n",
        "    if SELECTED_CANDIDATE_WHILE.lower() == 'quit':\n",
        "        break\n",
        "    if SELECTED_CANDIDATE_WHILE not in candidate_params:\n",
        "        print(\"Invalid candidate name. Please try again.\")\n",
        "        continue\n",
        "\n",
        "    user_question_WHILE = input(\"Enter the policy or stance you are interested in learning more about (or 'quit' to stop): \")\n",
        "    if user_question_WHILE.lower() == 'quit':\n",
        "        break\n",
        "    selected_params = candidate_params[SELECTED_CANDIDATE_WHILE]\n",
        "    selected_candidate_fullname_WHILE = candiate_lookup[SELECTED_CANDIDATE_WHILE][\"fullName\"]\n",
        "\n",
        "    user_question_formatted = USER_QUESTION.format(selected_candidate_fullname_WHILE, user_question_WHILE)\n",
        "\n",
        "    df_WHILE = pd.read_csv(\"distances/{}_distances.csv\".format(SELECTED_CANDIDATE_WHILE), index_col=0)\n",
        "\n",
        "    token_count_WHILE = len(tokenizer.encode(prompt_WHILE_with_data)) + len(tokenizer.encode(user_question_formatted))\n",
        "\n",
        "    context_list_WHILE = []\n",
        "\n",
        "    for text in df_WHILE[\"text\"].values:\n",
        "        token_count_WHILE += len(tokenizer.encode(text))\n",
        "        if token_count_WHILE <= token_limit:\n",
        "            context_list_WHILE.append(text)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    prompt_WHILE = prompt_WHILE_with_data.format(\"\\n\\n###\\n\\n\".join(context_list_WHILE), user_question_formatted)\n",
        "\n",
        "    prompt_answer_WHILE = openai.Completion.create(model=COMPLETION_MODEL_NAME, prompt=prompt_WHILE, max_tokens=150)\n",
        "\n",
        "    answer_WHILE = prompt_answer_WHILE[\"choices\"][0][\"text\"].strip()\n",
        "    print(answer_WHILE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example #6: While Loop\n",
        "Code output is candidate Joe Biden asked about Foreign Policy trained on the vector database in /distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joe Biden has said he is against regime change, but for providing non-military support to opposition movements. He has pledged to end U.S. support for the Saudi Arabian-led intervention in Yemen and to reevaluate the United States' relationship with Saudi Arabia. Biden supports extending the New START arms control treaty with Russia to limit the number of nuclear weapons deployed by both sides. He has spoken about human rights abuses in the Xinjiang region to the Chinese Communist Party leader Xi Jinping, pledging to sanction and commercially restrict Chinese government officials and entities who carry out repression. Biden has also endorsed a change to the Senate filibuster to allow for the passing of the Freedom to Vote Act and John Lewis Voting Rights Act, and he has led the U.S.\n"
          ]
        }
      ],
      "source": [
        "prompt_WHILE_with_data = \"\"\"\n",
        "Answer the question based on the context below about the candidate's political stance, and if the question can't be answered based on the context, say \"Hmmm...I haven't heard their stance on this. Sorry!\"\n",
        "\n",
        "Context: \n",
        "\n",
        "{}\n",
        "\n",
        "---\n",
        "\n",
        "Question: {}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    SELECTED_CANDIDATE_WHILE = input(\"Enter the name of the candidate (or 'quit' to stop): \")\n",
        "    if SELECTED_CANDIDATE_WHILE.lower() == 'quit':\n",
        "        break\n",
        "    if SELECTED_CANDIDATE_WHILE not in candidate_params:\n",
        "        print(\"Invalid candidate name. Please try again.\")\n",
        "        continue\n",
        "\n",
        "    user_question_WHILE = input(\"Enter the policy or stance you are interested in learning more about (or 'quit' to stop): \")\n",
        "    if user_question_WHILE.lower() == 'quit':\n",
        "        break\n",
        "    selected_params = candidate_params[SELECTED_CANDIDATE_WHILE]\n",
        "    selected_candidate_fullname_WHILE = candiate_lookup[SELECTED_CANDIDATE_WHILE][\"fullName\"]\n",
        "\n",
        "    user_question_formatted = USER_QUESTION.format(selected_candidate_fullname_WHILE, user_question_WHILE)\n",
        "\n",
        "    df_WHILE = pd.read_csv(\"distances/{}_distances.csv\".format(SELECTED_CANDIDATE_WHILE), index_col=0)\n",
        "\n",
        "    token_count_WHILE = len(tokenizer.encode(prompt_WHILE_with_data)) + len(tokenizer.encode(user_question_formatted))\n",
        "\n",
        "    context_list_WHILE = []\n",
        "\n",
        "    for text in df_WHILE[\"text\"].values:\n",
        "        token_count_WHILE += len(tokenizer.encode(text))\n",
        "        if token_count_WHILE <= token_limit:\n",
        "            context_list_WHILE.append(text)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    prompt_WHILE = prompt_WHILE_with_data.format(\"\\n\\n###\\n\\n\".join(context_list_WHILE), user_question_formatted)\n",
        "\n",
        "    prompt_answer_WHILE = openai.Completion.create(model=COMPLETION_MODEL_NAME, prompt=prompt_WHILE, max_tokens=150)\n",
        "\n",
        "    answer_WHILE = prompt_answer_WHILE[\"choices\"][0][\"text\"].strip()\n",
        "    print(answer_WHILE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MIT License\n",
        "\n",
        "### Copyright (c) 2023 @BrianHHough/Tech Stack Playbook¬ÆÔ∏è\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "\n",
        "NO WARRANTIES FOR OUTPUT: The output, responses, or answers provided by this Software, either through direct interaction or indirect use, are provided without any warranties. The owner of the Software is not responsible for any inaccuracies, misinformation, or errors produced in the output, responses, or answers by the Software.\n",
        "\n",
        "USER RESPONSIBILITY: The user of the Software acknowledges and agrees that the user is solely responsible for the interpretation and use of any output, responses, or answers provided by the Software."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
